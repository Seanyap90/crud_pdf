version: '3.8'

services:
  worker:
    build:
      context: ../../  # Navigate to project root
      dockerfile: src/files_api/vlm/Dockerfile
    # Load environment from .env.aws file created by deploy.py
    env_file:
      - ../../.env.aws
    environment:
      # Core deployment configuration
      - DEPLOYMENT_MODE=aws-mock
      
      # AWS Configuration
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-us-east-1}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-mock}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-mock}
      # Use host.docker.internal for container-to-host communication
      - AWS_ENDPOINT_URL=http://host.docker.internal:5000
      
      # S3 Configuration
      - S3_BUCKET_NAME=${S3_BUCKET_NAME:-rag-pdf-storage}
      
      # SQS Configuration
      - SQS_QUEUE_NAME=${SQS_QUEUE_NAME:-rag-task-queue}
      # Override the queue URL for docker environment
      - SQS_QUEUE_URL=http://host.docker.internal:5000/queue/${SQS_QUEUE_NAME:-rag-task-queue}
      
      # Model Configuration (from settings/env)
      - MODEL_MEMORY_LIMIT=${MODEL_MEMORY_LIMIT:-7GiB}
      - DISABLE_DUPLICATE_LOADING=${DISABLE_DUPLICATE_LOADING:-true}
      
      # Memory management flags
      - MAX_CONTENT_LENGTH=256
      - LOW_MEMORY=true
      - OFFLOAD_TO_CPU=true
      - CPU_OFFLOAD_FOLDER=/app/offload_folder
      - CACHE_IMPLEMENTATION=offloaded
      
      # CUDA optimization
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,garbage_collection_threshold:0.8
      
      # CPU threading optimization
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=6
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # Transformers cache location
      - TRANSFORMERS_CACHE=/app/cache
      - HF_HOME=/app/cache

    deploy:
      resources:
        reservations:
          # Reserve all GPU resources
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # Limit container memory to avoid OOM
          memory: 26G 
    
    # Improved container performance for high-res processing
    shm_size: 8gb
    cpuset: "0-11"
    mem_swappiness: 60
    ulimits:
      memlock: -1  # Unlimited memlock for CUDA operations
    restart: unless-stopped
    
    # Healthcheck with longer timeout for high-res processing
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep python | grep -v grep || exit 1"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 300s
    
    runtime: nvidia
    
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge