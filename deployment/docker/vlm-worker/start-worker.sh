#!/bin/bash
cd /app

# Make sure Python path is set
export PYTHONPATH=/app/src:$PYTHONPATH

# Function to check if models are available
check_models_available() {
    local cache_dir="/app/cache"
    
    echo "üîç Checking for required models in cache directory: $cache_dir"
    
    # Check for ColPali model
    local colpali_path="${cache_dir}/models--vidore--colpali"
    # Check for SmolVLM model
    local smolvlm_path="${cache_dir}/models--HuggingFaceTB--SmolVLM-Instruct"
    
    if [ -d "$colpali_path" ] && [ -d "$smolvlm_path" ]; then
        echo "‚úÖ All required models found in cache"
        echo "   - ColPali: $colpali_path"
        echo "   - SmolVLM: $smolvlm_path"
        return 0
    else
        echo "‚ùå Required models missing:"
        [ ! -d "$colpali_path" ] && echo "   - ColPali: NOT FOUND"
        [ ! -d "$smolvlm_path" ] && echo "   - SmolVLM: NOT FOUND"
        return 1
    fi
}

# Function to wait for models to be available (with timeout)
wait_for_models() {
    local max_wait=300  # 5 minutes
    local wait_time=0
    local check_interval=10
    
    echo "‚è≥ Waiting for models to be downloaded by model-downloader service..."
    
    while [ $wait_time -lt $max_wait ]; do
        if check_models_available; then
            echo "üéâ Models are ready!"
            return 0
        fi
        
        echo "‚è±Ô∏è  Models not ready yet, waiting... (${wait_time}s/${max_wait}s)"
        sleep $check_interval
        wait_time=$((wait_time + check_interval))
    done
    
    echo "üí• Timeout waiting for models after ${max_wait} seconds"
    echo "üí° The model-downloader service may have failed"
    echo "üí° Check logs: docker logs model-downloader"
    return 1
}

# Initialize GPU configuration and storage adapter
echo "üîß Initializing GPU configuration and storage adapter..."
python3 -c "
import sys
sys.path.append('/app/src')

from files_api.adapters.storage import init_storage
from vlm_workers.gpu.gpu_config import GPUConfigManager
import os

# Initialize GPU configuration first
print('üéØ Initializing GPU configuration...')
gpu_config = GPUConfigManager()
deployment_mode = gpu_config.validate_deployment_mode()
print(f'‚úì GPU configuration initialized for mode: {deployment_mode}')

# Apply GPU-specific environment overrides
gpu_config.apply_environment_overrides()
print('‚úì GPU environment overrides applied')

# Log GPU configuration for debugging
print(f'GPU Config:')
print(f'  - Model memory limit: {gpu_config.get_memory_config()}')
print(f'  - CUDA allocator: {gpu_config.get_cuda_allocator_config()}')
print(f'  - Cache implementation: {gpu_config.get_cache_config()}')
print(f'  - CPU offloading: {gpu_config.should_offload_to_cpu()}')
print(f'  - Use quantization: {gpu_config.config[\"use_quantization\"]}')

# Initialize storage adapter
mode = os.environ.get('QUEUE_TYPE', 'aws-mock')
print(f'Initializing storage in {mode} mode...')
init_storage(mode)
print('‚úì Storage adapter initialized')
"

# Wait for models to be available
if ! wait_for_models; then
    echo "üö® Cannot start worker without required models"
    exit 1
fi

# Set HF_HUB_OFFLINE for inference (models should already be cached)
export HF_HUB_OFFLINE=1
echo "üîí Set HF_HUB_OFFLINE=1 for offline model usage"

# Import worker and queue
echo "üöÄ Starting VLM+RAG Worker..."
python3 -c "
import asyncio
import sys
import os
sys.path.append('/app/src')

# Import worker and queue  
from vlm_workers.worker import Worker
from files_api.msg_queue import QueueFactory

print('üéØ Initializing worker components...')
queue = QueueFactory.get_queue_handler()
worker = Worker(queue)
print('‚úÖ Worker ready to process PDF inference tasks')
print('üîÑ Listening for tasks from SQS queue...')
asyncio.run(worker.listen_for_tasks())
"