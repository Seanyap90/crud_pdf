#!/bin/bash
cd /app

# Make sure Python path is set
export PYTHONPATH=/app/src:$PYTHONPATH

# Function to check if models are available and have proper structure
check_models_available() {
    local cache_dir="/app/cache"

    echo "ğŸ” Checking for required models in cache directory: $cache_dir"

    # Check for ColPali model (standard HuggingFace cache structure)
    local colpali_path="${cache_dir}/models--vidore--colpali"
    # Check for SmolVLM model
    local smolvlm_path="${cache_dir}/models--HuggingFaceTB--SmolVLM-Instruct"

    local models_valid=true

    # ColPali validation
    if [ -d "$colpali_path" ] && [ -d "$colpali_path/snapshots" ] && [ "$(ls -A $colpali_path/snapshots 2>/dev/null)" ]; then
        echo "âœ… ColPali model found and validated: $colpali_path"
    else
        echo "âŒ ColPali model missing or corrupted: $colpali_path"
        models_valid=false
    fi

    # SmolVLM validation
    if [ -d "$smolvlm_path" ] && [ -d "$smolvlm_path/snapshots" ] && [ "$(ls -A $smolvlm_path/snapshots 2>/dev/null)" ]; then
        echo "âœ… SmolVLM model found and validated: $smolvlm_path"
    else
        echo "âŒ SmolVLM model missing or corrupted: $smolvlm_path"
        models_valid=false
    fi

    if [ "$models_valid" = true ]; then
        echo "âœ… All required models validated successfully"
        return 0
    else
        echo "âŒ One or more models failed validation"
        return 1
    fi
}

# Function to refresh corrupted models using huggingface-cli
refresh_models_with_cli() {
    local cache_dir="/app/cache"
    echo "ğŸ”§ Attempting to refresh models using huggingface-cli..."

    # Temporarily disable offline mode for downloads
    export HF_HUB_OFFLINE=0

    # Models to refresh
    local models=(
        "HuggingFaceTB/SmolVLM-Instruct"
        "vidore/colpali"
        "vidore/colpaligemma-3b-mix-448-base"
    )

    for model in "${models[@]}"; do
        echo "ğŸ“¥ Refreshing $model using huggingface-cli..."
        if command -v huggingface-cli >/dev/null 2>&1; then
            huggingface-cli download "$model" \
                --cache-dir "$cache_dir" \
                --local-dir-use-symlinks False \
                --resume-download
        else
            echo "âš ï¸  huggingface-cli not found, using Python downloader as fallback..."
            python3 -m vlm_workers.models.downloader --cache-dir "$cache_dir"
        fi

        if [ $? -eq 0 ]; then
            echo "âœ… Successfully refreshed $model"
        else
            echo "âŒ Failed to refresh $model"
        fi
    done

    echo "ğŸ‰ Model refresh completed!"
}

# Function to wait for models to be available (with corruption detection)
wait_for_models() {
    local max_wait=300  # 5 minutes
    local wait_time=0
    local check_interval=10

    echo "â³ Checking for pre-loaded models..."

    # First check if models are already available
    if check_models_available; then
        echo "ğŸ‰ Pre-loaded models are ready!"
        return 0
    fi

    echo "âŒ Pre-loaded models are missing or corrupted"
    echo "ğŸ”§ Attempting to refresh models directly..."

    # Try to refresh models using huggingface-cli
    refresh_models_with_cli

    # Check again after refresh attempt
    if check_models_available; then
        echo "ğŸ‰ Models are now ready after refresh!"
        return 0
    else
        echo "ğŸ’¥ Failed to refresh models"
        echo "ğŸ’¡ Check network connectivity and HuggingFace Hub access"
        return 1
    fi
}

# Initialize storage adapter with environment mode
echo "ğŸ”§ Initializing storage adapter..."
python3 -c "
import sys
sys.path.append('/app/src')

from files_api.adapters.storage import init_storage
import os

mode = os.environ.get('QUEUE_TYPE', 'aws-mock')
print(f'Initializing storage in {mode} mode...')
init_storage(mode)
print('âœ“ Storage adapter initialized')
"

# Wait for models to be available
if ! wait_for_models; then
    echo "ğŸš¨ Cannot start worker without required models"
    exit 1
fi

# Set HF_HUB_OFFLINE for inference (models should already be cached)
export HF_HUB_OFFLINE=1
echo "ğŸ”’ Set HF_HUB_OFFLINE=1 for offline model usage"

# Import worker and queue
echo "ğŸš€ Starting VLM+RAG Worker..."
python3 -c "
import asyncio
import sys
import os
sys.path.append('/app/src')

# Import worker and queue  
from vlm_workers.worker import Worker
from files_api.msg_queue import QueueFactory

print('ğŸ¯ Initializing worker components...')
queue = QueueFactory.get_queue_handler()
worker = Worker(queue)
print('âœ… Worker ready to process PDF inference tasks')
print('ğŸ”„ Listening for tasks from SQS queue...')
asyncio.run(worker.listen_for_tasks())
"